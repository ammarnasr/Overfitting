{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(<numpy.lib.npyio.NpzFile object at 0x0000020324F63BE0>)\n",
      "KeysView(<numpy.lib.npyio.NpzFile object at 0x0000020324F6B2B0>)\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, DropoutLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
    "from mlp.penalties import L1Penalty, L2Penalty\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(model, error, learning_rule, train_data, valid_data,\n",
    "                                num_epochs, stats_interval, notebook=True, plots=False):\n",
    "    \n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "    optimiser = Optimiser(model, error, learning_rule, train_data,\n",
    "                          valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    if plots:\n",
    "        fig_1 = plt.figure(figsize=(8, 4))\n",
    "        ax_1 = fig_1.add_subplot(111)\n",
    "        for k in ['error(train)', 'error(valid)']:\n",
    "            ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                    stats[1:, keys[k]], label=k)\n",
    "        ax_1.legend(loc=0)\n",
    "        ax_1.set_xlabel('Epoch number')\n",
    "        ax_1.set_ylabel('Error')\n",
    "        fig_2 = plt.figure(figsize=(8, 4))\n",
    "        ax_2 = fig_2.add_subplot(111)\n",
    "        for k in ['acc(train)', 'acc(valid)']:\n",
    "            ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                    stats[1:, keys[k]], label=k)\n",
    "        ax_2.legend(loc=0)\n",
    "        ax_2.set_xlabel('Epoch number')\n",
    "        ax_2.set_xlabel('Accuracy')\n",
    "        return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2\n",
    "    else:\n",
    "        return stats, keys, run_time\n",
    "\n",
    "def get_logger (filename = 'logs.log'):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "    # logger.handlers = [logging.StreamHandler()]\n",
    "    # create file handler which logs even debug messages\n",
    "    fh = logging.FileHandler(filename)\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "def get_pre_defined_params ():\n",
    "    seed = 11102019 \n",
    "    rng = np.random.RandomState(seed)\n",
    "    batch_size = 100\n",
    "    logger = get_logger()\n",
    "    train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "    valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 20\n",
    "    stats_interval = 1\n",
    "    input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)  \n",
    "    return rng, batch_size, train_data, valid_data, learning_rate, stats_interval , input_dim, output_dim, hidden_dim, weights_init, biases_init\n",
    "\n",
    "\n",
    "rng, batch_size, train_data, valid_data, learning_rate, stats_interval , input_dim,\\\n",
    "     output_dim, hidden_dim, weights_init, biases_init = get_pre_defined_params()\n",
    "logger = get_logger()\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "setups = [\n",
    "    (None, None, DropoutLayer(incl_prob=0.7)),\n",
    "    (L1Penalty(1e-3), None, DropoutLayer(incl_prob=1)),\n",
    "    (None, L2Penalty(1e-3), DropoutLayer(incl_prob=1)),\n",
    "]\n",
    "num_epochs = 100\n",
    "filename = f'Task2_using_{num_epochs}_epoch_{learning_rate}_LR.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1,l2,do in setups:\n",
    "    \n",
    "    l = None\n",
    "    if l1 is not None:\n",
    "        l = l1\n",
    "    if l2 is not None:\n",
    "        l = l2\n",
    "    \n",
    "    train_data.reset()  \n",
    "    valid_data.reset()  \n",
    "    result_key = f'l_{l}-l1_{l1}-l2_{l2}-dropout_{do}-epochs_{num_epochs}-lr_{learning_rate}'\n",
    "    Logger = get_logger(f'{result_key}.log')\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty= l, biases_penalty= l), # first hidden layer\n",
    "        ReluLayer(),\n",
    "        do,\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty= l, biases_penalty= l), # second hidden layer\n",
    "        ReluLayer(),\n",
    "        do,\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty= l, biases_penalty= l), # Third hidden layer\n",
    "        ReluLayer(),\n",
    "        do,\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty= l, biases_penalty= l) # output layer\n",
    "    ])\n",
    "    \n",
    "    r= train_model_and_plot_stats(\n",
    "            model, error, learning_rule, train_data,\n",
    "            valid_data, num_epochs, stats_interval, notebook=True, plots=True)\n",
    "    results_dict[result_key] = r\n",
    "    \n",
    "np.save(filename, results_dict) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('Task2_using_2_epoch_0.0001_LR.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "train_data.reset()  \n",
    "valid_data.reset()  \n",
    "l1 = None #L1Penalty(0)\n",
    "l2 = None #L2Penalty(0)\n",
    "do = DropoutLayer(incl_prob= 0.97)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty= l2, biases_penalty= l2), # first hidden layer\n",
    "    ReluLayer(),\n",
    "    do,\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty= l2, biases_penalty= l2), # second hidden layer\n",
    "    ReluLayer(),\n",
    "    do,\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty= l2, biases_penalty= l2), # Third hidden layer\n",
    "    ReluLayer(),\n",
    "    do,\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty= l2, biases_penalty= l2) # output layer\n",
    "])\n",
    "_= train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data,\n",
    "        valid_data, num_epochs, stats_interval, notebook=True, plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('my_file.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 15\n",
    "# train_data.reset()  \n",
    "# valid_data.reset()  \n",
    "# l1 = L1Penalty(0.5)\n",
    "# l2 = L2Penalty(0.5)\n",
    "# do = DropoutLayer(incl_prob= 0.5)\n",
    "# model = MultipleLayerModel([\n",
    "#     AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\n",
    "#     ReluLayer(),\n",
    "#     do,\n",
    "#     AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\n",
    "#     ReluLayer(),\n",
    "#     do,\n",
    "#     AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # Third hidden layer\n",
    "#     ReluLayer(),\n",
    "#     do,\n",
    "#     AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "# ])\n",
    "# _= train_model_and_plot_stats(\n",
    "#         model, error, learning_rule, train_data,\n",
    "#         valid_data, num_epochs, stats_interval, notebook=True, plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "train_data.reset()  \n",
    "valid_data.reset()  \n",
    "l1 = L1Penalty(0.001)\n",
    "l2 = L2Penalty(0.001)\n",
    "do = DropoutLayer(incl_prob= 1)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty= l1, biases_penalty= l1), # first hidden layer\n",
    "    ReluLayer(),\n",
    "    do,\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty= l1, biases_penalty= l1), # second hidden layer\n",
    "    ReluLayer(),\n",
    "    do,\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty= l1, biases_penalty= l1), # Third hidden layer\n",
    "    ReluLayer(),\n",
    "    do,\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty= l1, biases_penalty= l1) # output layer\n",
    "])\n",
    "_= train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data,\n",
    "        valid_data, num_epochs, stats_interval, notebook=True, plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "train_data.reset()  \n",
    "valid_data.reset()  \n",
    "l1 = L1Penalty(0.001)\n",
    "l2 = L2Penalty(0.001)\n",
    "do = DropoutLayer(incl_prob= 1)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty= l2, biases_penalty= l2), # first hidden layer\n",
    "    ReluLayer(),\n",
    "    do,\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty= l2, biases_penalty= l2), # second hidden layer\n",
    "    ReluLayer(),\n",
    "    do,\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty= l2, biases_penalty= l2), # Third hidden layer\n",
    "    ReluLayer(),\n",
    "    do,\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty= l2, biases_penalty= l2) # output layer\n",
    "])\n",
    "_= train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data,\n",
    "        valid_data, num_epochs, stats_interval, notebook=True, plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11102019 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "logger = get_logger()\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "exp2_res = []\n",
    "net_depth = [1,2,3]\n",
    "train_data.reset()  \n",
    "valid_data.reset()  \n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)  \n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # first hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # second hidden layer\n",
    "    ReluLayer(),\n",
    "    # DropoutLayer(rng = rng, incl_prob=0.5),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "# stats, keys, _ = train_model_and_plot_stats(\n",
    "#         model, error, learning_rule, train_data,\n",
    "#         valid_data, num_epochs, stats_interval, notebook=True, plots=False)\n",
    "_= train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data,\n",
    "        valid_data, num_epochs, stats_interval, notebook=True, plots=True)\n",
    "exp2_res.append(stats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11102019 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "logger = get_logger()\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "exp2_res = []\n",
    "net_depth = [1,2,3]\n",
    "train_data.reset()  \n",
    "valid_data.reset()  \n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)  \n",
    "l1 = L1Penalty(0.1)\n",
    "l2 = L2Penalty(0.1)\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init,\n",
    "     weights_penalty=None, biases_penalty=None),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init,\n",
    "     weights_penalty=None, biases_penalty=None), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init,\n",
    "     weights_penalty=l1, biases_penalty=l1),\n",
    "    ReluLayer(),\n",
    "    # DropoutLayer(rng = rng, incl_prob=0.5),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init) # output layer\n",
    "])\n",
    "error = CrossEntropySoftmaxError()\n",
    "learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "# stats, keys, _ = train_model_and_plot_stats(\n",
    "#         model, error, learning_rule, train_data,\n",
    "#         valid_data, num_epochs, stats_interval, notebook=True, plots=False)\n",
    "_= train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data,\n",
    "        valid_data, num_epochs, stats_interval, notebook=True, plots=True)\n",
    "exp2_res.append(stats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15a1108c086910f85ca4baff738b45dae52df791633b9cd1b62a05e2975e2a2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
